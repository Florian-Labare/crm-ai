# ğŸ¤ Whisper Local - Transcription Audio

Ce dossier contient le script Python pour la transcription audio locale avec **faster-whisper**.

## ğŸ“¦ Installation

Les dÃ©pendances Python sont automatiquement installÃ©es lors du build du container Docker backend.

Si vous souhaitez tester en local (hors Docker) :

```bash
pip install -r requirements.txt
```

## ğŸš€ Utilisation

### Via Docker (automatique)

Le script est automatiquement appelÃ© par Laravel via `TranscriptionService.php` lors de l'upload d'un fichier audio.

### Test manuel

```bash
python3 whisper_transcribe.py <chemin_fichier_audio> [modele]
```

**Exemples :**

```bash
# ModÃ¨le base (par dÃ©faut)
python3 whisper_transcribe.py /path/to/audio.wav

# ModÃ¨le small (meilleure qualitÃ©)
python3 whisper_transcribe.py /path/to/audio.mp3 small

# ModÃ¨le tiny (plus rapide)
python3 whisper_transcribe.py /path/to/audio.wav tiny
```

## ğŸ¯ ModÃ¨les disponibles

| ModÃ¨le | Taille | Vitesse | QualitÃ© | Recommandation |
|--------|--------|---------|---------|----------------|
| `tiny` | ~75 MB | âš¡âš¡âš¡âš¡âš¡ | â­â­ | Tests rapides |
| `base` | ~150 MB | âš¡âš¡âš¡âš¡ | â­â­â­ | **POC (recommandÃ©)** |
| `small` | ~500 MB | âš¡âš¡âš¡ | â­â­â­â­ | Production |
| `medium` | ~1.5 GB | âš¡âš¡ | â­â­â­â­â­ | Haute qualitÃ© |
| `large-v3` | ~3 GB | âš¡ | â­â­â­â­â­ | Maximum qualitÃ© |

## ğŸ”§ Configuration

Le modÃ¨le utilisÃ© est dÃ©fini dans `.env` :

```env
WHISPER_MODEL=base
```

## ğŸ“Š Format de sortie

Le script retourne un JSON :

```json
{
  "text": "Transcription du fichier audio...",
  "language": "fr",
  "language_probability": 0.98
}
```

En cas d'erreur :

```json
{
  "error": "Message d'erreur..."
}
```

## ğŸ› Troubleshooting

### Erreur : `ModuleNotFoundError: No module named 'faster_whisper'`

RÃ©installer les dÃ©pendances :

```bash
docker compose exec backend pip3 install -r /var/www/html/scripts/requirements.txt
```

### Le modÃ¨le tÃ©lÃ©charge Ã  chaque fois

Les modÃ¨les Whisper sont mis en cache dans `~/.cache/huggingface/`. Pour persister ce cache dans Docker, ajouter un volume dans `docker-compose.yml`.

### Performances lentes

- Utiliser un modÃ¨le plus petit (`tiny` ou `base`)
- Activer VAD (Voice Activity Detection) - dÃ©jÃ  activÃ© par dÃ©faut
- Si vous avez un GPU NVIDIA, modifier le script pour utiliser `device="cuda"`

## ğŸ“ Avantages vs API OpenAI

âœ… **Gratuit** (pas de coÃ»t API)
âœ… **PrivÃ©** (donnÃ©es ne quittent pas le serveur)
âœ… **Rapide** (pas de latence rÃ©seau)
âœ… **Offline** (fonctionne sans internet)
âš ï¸ **ModÃ¨le base** : qualitÃ© lÃ©gÃ¨rement infÃ©rieure Ã  l'API mais suffisant pour POC

## ğŸ”— Liens utiles

- [faster-whisper Documentation](https://github.com/SYSTRAN/faster-whisper)
- [OpenAI Whisper](https://github.com/openai/whisper)
- [Hugging Face Models](https://huggingface.co/models?search=whisper)
